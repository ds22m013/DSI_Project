{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170ade2a-e1ee-43e6-b71b-5ac3d2944a25",
   "metadata": {},
   "source": [
    "# **HOLIDAY PLANNER**\n",
    "By:\n",
    "* Jorge Luis Perez Tortosa\n",
    "* Özgür Sahin\n",
    "* Bernhard Wieland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2423a97-e6f2-47a8-a489-556e4de82162",
   "metadata": {},
   "source": [
    "## **GENERAL INFORMATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82acf21-3973-46ca-8a7e-9f063290d353",
   "metadata": {},
   "source": [
    "### Application for -> Trip planning\n",
    "\n",
    "Finds best:\n",
    "- Flight <br>\n",
    "- 3 x hotels <br>\n",
    "- Monuments <br>\n",
    "\n",
    "for selected **country*** and next **4 days*** departing from Vienna.\n",
    "\n",
    "*The amount of countries and days is easily adaptable. It is restricted because of limited amount of free API requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388eee16-d5cc-41c7-b6ce-9195d4f56595",
   "metadata": {},
   "source": [
    "### ARCHITECTURE OVERVIEW\n",
    "\n",
    "![title](img/Architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cb324f-7d7e-4944-be7d-7a436f9d9059",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DATA SOURCES:\n",
    "\n",
    "- **Flight data:** it is received from the Skyscanner44 API in rapidapi.com. The API returns already the best flight by own criteria.\n",
    "https://rapidapi.com/3b-data-3b-data-default/api/skyscanner44\n",
    "\n",
    "- **Hotel data:** it is received from the Booking API in rapidapi.com\n",
    "https://rapidapi.com/tipsters/api/booking-com/\n",
    "\n",
    "- **Monument data:** data comes from a manually created CSV based on the book: 1000 Places to see before you die.\n",
    "\n",
    "- **Country data:** dataset from Kaggle with country information (capital, location...). Enhanced with booking_id (for booking API requests) and airport_code (for skyScanner API requests)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e31fb-0b2f-451c-8dad-412fac8127c4",
   "metadata": {},
   "source": [
    "### DATA STRUCTURE (after processing):\n",
    "- **Flight (x1 per date/city):**<br>\n",
    "each entry is the best flight for a destination for a certain date and in a certain import.<br>\n",
    "<ins>f_id:</ins> unique identifier assigned by DB.<br>\n",
    "<ins>date:</ins> departure date.<br>\n",
    "<ins>time:</ins> departure time.<br>\n",
    "<ins>arrival_time:</ins> arrival time.<br>\n",
    "<ins>origin:</ins> start city for the trip. In this project, always Vienna.<br>\n",
    "<ins>destination:</ins> final destination of the trip.<br>\n",
    "<ins>company:</ins> carrier operating the flight.<br>\n",
    "<ins>n_connections:</ins> number of connections needed for arriving the destination.<br>\n",
    "<ins>import_date:</ins> when was the data imported (used for filtering to most recent in Streamlit).\n",
    "\n",
    "- **Hotel (x3 per date/city)**: <br>\n",
    "each entry is one of the 3 best hotels in a city for a certain date and in a certain import.<br>\n",
    "<ins>h_id:</ins> unique identifier assigned by DB.<br>\n",
    "<ins>checkin_date:</ins> arrival date in hotel.<br>\n",
    "<ins>city:</ins> city where hotel is located.<br>\n",
    "<ins>name:</ins> name of the hotel.<br>\n",
    "<ins>price:</ins> daily price.<br>\n",
    "<ins>longitude:</ins> longitude coordinate of the hotel.<br>\n",
    "<ins>latitude:</ins> latitude coordinate of the hotel.<br>\n",
    "<ins>import_date:</ins> when was the data imported (used for filtering to most recent in Streamlit).<br>\n",
    "\n",
    "- **Monument**: <br>\n",
    "each entry represents one of the monuments in the book.<br>\n",
    "<ins> m_id:</ins> unique identifier assigned by DB.<br>\n",
    "<ins> name:</ins> name of the monument. <br>\n",
    "<ins> country:</ins> country where it is located. <br>\n",
    "<ins> longitude:</ins> longitude coordinate of the monument. <br>\n",
    "<ins> latitude:</ins> latitude coordinate of the monument. <br>\n",
    "\n",
    "- **Country**: <br>\n",
    "each city represents a country and its basic information. This data serves as a lookup table for API requests or Streamlit.<br>\n",
    "<ins> c_id:</ins> unique identifier assigned by DB. <br>\n",
    "<ins> capital:</ins> name of the country. <br>\n",
    "<ins> country:</ins> capital of the country <br>\n",
    "<ins> dest_id_booking:</ins> ID of the capital in the booking portal.\n",
    "<ins> airport_code:</ins> code of the airport in the capital."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b3cb0-b9f4-430d-9ca5-2237cd950240",
   "metadata": {},
   "source": [
    "### Current version GitHub:\n",
    "https://github.com/ds22m013/DSI_Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563a83fb-d18b-4a5c-a6b9-5bb68c422c7c",
   "metadata": {},
   "source": [
    "## **PROJECT CODE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc872716-92bb-4537-a603-883bf865efbf",
   "metadata": {},
   "source": [
    "In this part, the code of the project will be described with a step-by-step description on how to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a06171d-8816-4d53-bbe4-a24a8ed27338",
   "metadata": {},
   "source": [
    "### **MODULES**:\n",
    "\n",
    "In order to group the code thematically and not overload the docu, we have distributed key parts of the program in separate modules.\n",
    "Their contents are:\n",
    "\n",
    "#### **Key modules:**\n",
    "* **API_Requests:** contains the functions in charge of sending the API requests (separately) and parsing the responses. They return a pandasDF with the relevant response data.\n",
    "* **Kafka_Communication:** here are two functions included. One in charge of writing in a topic and the other one in charge of reading from a topic.\n",
    "* **transformationBooking:** here are the functions needed for transforming the booking data. Transformation description: <br>\n",
    "Transformation is done with SparkSQL. <br>\n",
    "The data of many hotels for different dates is read as input into a SparkDF. <br>\n",
    "It creates a rating for each date and city consisting of: 70% price and 30% rating. Then selects the best three ranked hotels for each day and location.<br>\n",
    "Returns the data converted in a pandasDF.<br>\n",
    "* **transformationSkyscanner:** here are the functions used for transforming the skyscanner data. Transformation description: <br>\n",
    "Transformation is done using MapReduce.<br>\n",
    "The data of one flight is read into a SparkDF and the columns duration and time are loaded into an RDD.<br>\n",
    "The arrival_date is calculated using a function which takes the duration and time as arguments.<br>\n",
    "Resulting data is appended to pandasDF and returned.<br>\n",
    "* **Places1000:** streamlit script.\n",
    "\n",
    "#### **Support modules:**\n",
    "* **config:** different variables needed throughout the program and subject to change are saved here.\n",
    "* **CSVLoader:** script used to create the database tables, read the monuments and capitalsCSV and load them in the database.\n",
    "* **DB_Scripts:** different functions in relation with the database. Retrieve and insert data, or create tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f445714f-c60d-4a3b-a377-ba90d2413fa4",
   "metadata": {},
   "source": [
    "### **STEP-BY-STEP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be0cc3b-6e10-4912-95e4-12602768b9d9",
   "metadata": {},
   "source": [
    "#### **1) SETUP BASIC INFRASTRUCTURE**\n",
    "\n",
    "* **Run Docker**: docker compose -f docker-compose.yaml up -d\n",
    "* **Create DB Basics**: run CSVLoader.create_start_DB()\n",
    "* **API Register**: register in the APIs and get the key.\n",
    "* **Config**: adapt the values in the config.py module to your personal setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0d1de-42d2-4769-b947-679db0053b13",
   "metadata": {},
   "source": [
    "#### **2) SERVER 1: CREATE REQUESTS + PRODUCE IN KAFKA**\n",
    "\n",
    "This part is a loop which every hour:\n",
    "- **Skyscanner**:\n",
    "    * Sends API Request for every country in DB and days specified in config.\n",
    "    * Writes response in Kafka topic.\n",
    "- **Booking**:\n",
    "    * Sends API Request for every country in DB and days specified in config.\n",
    "    * Writes response in Kafka topic.\n",
    "\n",
    "**Key modules involved:** API_Request, Kafka_Communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ae89f-7371-416d-bf1a-73a065e46682",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## IMPORTS\n",
    "import API_Requests\n",
    "import DB_Scripts\n",
    "import Kafka_Communication\n",
    "import transformationBooking\n",
    "import transformationSkyscanner\n",
    "from datetime import date, timedelta, datetime\n",
    "import pandas as pd\n",
    "import config\n",
    "from time import sleep\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723019b-c927-4b5e-8fb5-1c86eb382999",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SERVER 1) REQUESTS + PRODUCE IN KAFKA\n",
    "\n",
    "refDate = date.today() + timedelta(days=1)\n",
    "countryData = DB_Scripts.get_country_data()\n",
    "dayData = [refDate + timedelta(days=i) for i in range(1, config.days_to_add)]\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        for index, row in countryData.iterrows():\n",
    "            for day in dayData:\n",
    "                ts1 = day.strftime(\"%Y-%m-%d\")\n",
    "                try:\n",
    "                    bookingData = API_Requests.get_BookingAPI_response(str(int(row[\"dest_id_booking\"])), ts1)\n",
    "                    Kafka_Communication.kafka_send(bookingData, config.kafka_topic_booking)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                try:\n",
    "                    flightData = API_Requests.get_SkyScannerAPI_response(row[2], row[1], ts1)\n",
    "                    Kafka_Communication.kafka_send(flightData, config.kafka_topic_skyscanner)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "        print(\"Loading complete\") \n",
    "        sleep(3600)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Loading stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc529954-8a22-4787-858e-ee24d6b32f63",
   "metadata": {},
   "source": [
    "#### **3) SERVER 2: CONSUME KAFKA + WRITE IN DB**\n",
    "This part is a loop which every hour:\n",
    "\n",
    "- **Skyscanner**:\n",
    "    * Reads all the entries in the Kafka topic.\n",
    "    * Does a transformation using RDD.\n",
    "    * Writes result in PostgreSQL.\n",
    "- **Booking**:\n",
    "    * Reads all the entries in the Kafka topic.\n",
    "    * Does a transformation using SparkSQL.\n",
    "    * Writes result in PostgreSQL.\n",
    "    \n",
    "**Key modules involved:** Kafka_Communication, transformationBooking, transformationSkyscanner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6765b332-fb78-4315-96d2-fa67c444e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SERVER 2) CONSUME KAFKA -> SPARK (MANIPULATE) -> SAVE IN DB\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(config.spark_master) \\\n",
    "    .appName(config.spark_app_name) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        bookingDF = Kafka_Communication.kafka_receive(config.kafka_topic_booking)\n",
    "        if (bookingDF.empty == False):\n",
    "            bookingDF = transformationBooking.transform_booking_inSpark(spark, bookingDF)\n",
    "            DB_Scripts.send_booking_to_DB(bookingDF)\n",
    "        flightsDF = Kafka_Communication.kafka_receive(config.kafka_topic_skyscanner)\n",
    "        if (flightsDF.empty == False):\n",
    "            flightsDF = transformationSkyscanner.transform_Skyscanner_RDD(spark, flightsDF)\n",
    "            DB_Scripts.send_skyscanner_to_DB(flightsDF)\n",
    "        print(\"Loading complete\")\n",
    "        sleep(3600)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Loading stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5fda8f-6b4d-418d-8e95-4183a46cdb69",
   "metadata": {},
   "source": [
    "#### **4) RUN STREAMLIT**\n",
    "Use in the terminal in this folder: streamlit run Places1000.py<br>\n",
    "\n",
    "The applications welcomes the user.<br>\n",
    "The user selects the country where he wants to travel in the selectbox of the left side.<br>\n",
    "-> User gets flight information for the next 4 days.\n",
    "\n",
    "![title](img/Streamlit1.png)\n",
    "\n",
    "The user selects one of the four days in the selectbox on the left side.<br>\n",
    "-> User gets displayed in a map the hotels and monument locations (with labels).<br>\n",
    "\n",
    "![title](img/Streamlit2.png)\n",
    "\n",
    "-> User gets an overview of the trip he selected (flight info, best three hotels and monument names).<br>\n",
    "\n",
    "![title](img/Streamlit3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6297c9-ecf7-4f5f-9a56-5e783e93b4e3",
   "metadata": {},
   "source": [
    "## REQUIREMENTS OVERVIEW\n",
    "\n",
    "* **Connection to multiple data sources**: YES. We use 2 APIs (Skyscanner, Booking) and 2 CSVs (monuments, country).\n",
    "* **Data storage**: YES. We use a PostgreSQL database to save our data.\n",
    "* **Kafka**: YES. We produce and consume data from 2 topics (one per API).\n",
    "* **MapReduce with Spark**: YES. We use it for transforming the Skyscanner data.\n",
    "* **SparkSQL and SparkDF**: YES. We use it for transforming the Booking data.\n",
    "* **Visualisation**: YES. Covered in Streamlit with a chart and a map.\n",
    "* **Documentation**: YES. You are reading it.\n",
    "* **GIT**: YES. Link above.\n",
    "* **NoSQL DB**: YES. ElasticSearch: in separate file.\n",
    "* **Presentation, Punctuality**: YES. All the requirements were delivered before due date. Presentation was held."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
